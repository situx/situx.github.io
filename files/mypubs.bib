@article{Prudhomme2019,
author={Prudhomme, Claire and Homburg, Timo and Ponciano, Jean-Jacques and Boochs, Frank and Cruz, Christophe and Roxin, Ana-Maria},
title={Interpretation and automatic integration of geospatial data into the Semantic Web},
journal={Computing},
year={2019},
month={February},
day={13},
pages={1--27},
publisher={Springer International Publishing},
address={Cham, Switzerland},
language={english},
abstract={In the context of disaster management, geospatial information plays a crucial role in the decision-making process to protect and save the population. Gathering a maximum of information from different sources to oversee the current situation is a complex task due to the diversity of data formats and structures. Although several approaches have been designed to integrate data from different sources into an ontology, they mainly require background knowledge of the data. However, non-standard data set schema (NSDS) of relational geospatial data retrieved from e.g. web feature services are not always documented. This lack of background knowledge is a major challenge for automatic semantic data integration. Focusing on this problem, this article presents an automatic approach for geospatial data integration in NSDS. This approach does a schema mapping according to the result of an ontology matching corresponding to a semantic interpretation process. This process is based on geocoding and natural language processing. This article extends work done in a previous publication by an improved unit detection algorithm, data quality and provenance enrichments, the detection of feature clusters. It also presents an improved evaluation process to better assess the performance of this approach compared to a manually created ontology. These experiments have shown the automatic approach obtains an error of semantic interpretation around 10\% according to a manual approach.},
issn={1436-5057},
doi={10.1007/s00607-019-00701-y},
url={https://doi.org/10.1007/s00607-019-00701-y}
}

@inproceedings{homburg2018semgis,
  title={Semantic Geographic Information System: Integration and management of heterogeneous geodata},
  author={Homburg, Timo and Prudhomme, Claire and Boochs, Frank},
  booktitle = {Fachaustausch Geoinformation 2018},
  language={english},
  day={29},
  doi = {10.13140/RG.2.2.35246.56645},
  month={November},
  year={2018}
}

@InProceedings{homburg2018situation,
	author={Homburg, Timo and Boochs, Frank},
	editor={Abramowicz, Witold and Paschke, Adrian},
	title={Situation-Dependent Data Quality Analysis for Geospatial Data Using Semantic Technologies},
	booktitle={Business Information Systems Workshops},
	year={2019},
  month={January},
  day={3},
	publisher={Springer International Publishing},
	address={Cham, Switzerland},
	pages={566--578},
	 language={english},
	abstract={In this paper we present a new way to evaluate geospatial data quality using Semantic technologies. In contrast to non-semantic approaches to evaluate data quality, Semantic technologies allow us to model situations in which geospatial data may be used and to apply costumized geospatial data quality models using reasoning algorithms on a broad scale. We explain how to model data quality using common vocabularies of ontologies in various contexts, apply data quality results using reasoning in a real-world application case using OpenStreetMap as our data source and highlight the results of our findings on the example of disaster management planning for rescue forces. We contribute to the Semantic Web community and the OpenStreetMap community by proposing a semantic framework to combine usecase dependent data quality assignments which can be used as reasoning rules and as data quality assurance tools for both communities respectively.},
	isbn={978-3-030-04849-5},
  doi={10.1007/978-3-030-04849-5_49},
  url={https://link.springer.com/chapter/10.1007/978-3-030-04849-5_49}
}

@inproceedings{homburg2018semantische,
  title={Semantische Extraktion auf antiken Schriften am
Beispiel von Keilschriftsprachen mithilfe
semantischer Wörterbücher},
  author={Homburg, Timo},
  booktitle = {Extended Abstract Digital Humanities im deutschsprachigen Raum (DHd 2018)},
  language={german},
  abstract={Einleitung und Motivation
Semantische Extraktionsmechanismen (z.B. Topic Modelling) werden seit vielen Jahren im Bereich des Semantic Web und Natural Language
Processings sowie in den Digital Humanities als
Verfahren zur Visualisierung und automatischen
Kategorisierung von Dokumenten eingesetzt. Oft
ergeben sich durch den Einsatz neue Aspekte
der Interpretation von Dokumentensammlungen
die vorher noch nicht ersichtlich waren. Als Beispiele solcher Verfahren kommen häufig Machine
Learning Algorithmen zum Einsatz, welche eine
Grobeinordnung von Texten vornehmen können.
Gepaart mit Metadaten von Texten können anschließend beispielsweise thematische Übersichten von Dokumenten mit geographischem Bezug
auf Kartenmaterialien in GIS Systemen oder mittels historischer Gazetteers zeitliche Zusammenhänge automatisiert dargestellt werden. In dieser
Publikation möchten wir die Möglichkeiten der
semantischen Extraktion nutzen und diese auf ei464
Digital Humanities im deutschsprachigen Raum 2018
ner Sammlung von Texten in Keilschriftsprachen
anwenden.
Keilschriftsprachen
Keilschriftsprachen haben in den letzten Jahren ein größeres Interesse in der Digital Humanities und Linguistik Community erfahren. (Inglese 2015, Homburg et. al. 2016, Homburg 2017,
Sukhareva et. al. 2017). Neben der andauernden Standardisierung in Unicode werden unter
anderem Part Of Speech Tagger und Mechanismen der automatisierten Übersetzung erprobt um
Keilschrifttexte besser mit dem Computer zu erfassen und zu interpretieren. Desweiteren wurde
die Erlernbarkeit der Keilschriftsprachen durch
digitale Tools wie Eingabemethoden oder Karteikartenlernprogramme verbessert. (Homburg
2015) Trotz all der erreichten Fortschritte verbleiben jedoch zahlreiche Probleme bei der maschinellen Verarbeitung von Keilschriftsprachen, die
unter anderem mit der geringen Verfügbarkeit
annotierter Ressourcen und der fehlenden Verfügbarkeit maschinenlesbarer und semantisch
sowie linguistisch annotierter Wörterbücher zusammenhängt. Diese Limitierungen hindern viele
Natural Language Processing und semantische
Extraktionsalgorithmen daran ein besseres Ergebnis zu erzielen. Wir möchten mit dieser Publikation einen Beitrag leisten diese Situation zu
verbessern und stellen das "Semantic Dictionary
for Ancient Languages" vor, welches ein Versuch
ist durch Annotierung vorhandener in der Forschungscommunity anerkannter Wörterbuchressourcen mit Unicode Characters, Semantic Web
Konzepten, etymologischen Daten, gemeinsamen
Vokabularen und POSTags eine semantische Ressource in RDF für die Optimierung solcher Algorithmen auf Basis der Sprachen Hethitisch,
Sumerisch und Akkadisch zu schaffen.Das Wörterbuch basiert auf dem Lemon-Standard, ein
W3C Standard der es erlaubt ebenfalls multilinguale Resourcen abzubilden. So können Entwicklungen der Sprache und gemeinsame Vokabulare
wie zum Beispiel Akkadogramme und Sumerogramme in Hethitisch mit erfasst werden.
Semantisches Wörterbuch
und Semantische Extraktion
Wir testen die Performance des Wörterbuchs
auf einer der größten Sammlungen von digitalen Keilschrifttexten, der CDLI, aus der wir repräsentative Texte in hethitischer, sumerischer und
akkadischer Keilschrift aus verschiedenen Epochen extrahieren und mittels Machine Learning
klassifizieren, sowie verschlagworten. Das Ergebnis der semantischen Extraktion ist eine Sammlung von Themen pro Keilschrifttafel, die sich wiederum in Überkategorien gruppieren lassen und
in einen zeitlichen, sprachlichen, dialektischen,
sowie örtlichen Kontext gestellt werden können.
Anhand der verschiedenen Metadaten der CDLI
war es uns möglich eine thematische Karte der
Fundorte der Keilschrifttafeln sowie deren Inhalt
pro Epoche darzustellen aus der das relevante
Fachpublikum schließen kann welche Themen zu
welcher Zeit an welchem Fundort relevant für
die Schreiber der jeweiligen Epoche waren. Im
Zuge einer Weiterentwicklung möchten wir diese
Informationen mit weiteren Metadaten wie beispielsweise der Jurisdiktion, den Daten der jeweiligen Herrscher sowie rekonstruierten Orten
aus der antiken Zeit vervollständigen um Rückschlüsse auf interessante historische Ereignisse
zu ziehen.
Aufbau des Posters
Auf unserem Poster möchten wir gerne den
Prozess des Aufbaus, sowie die Struktur des semantischen Wörterbuchs sowie die Karte die
durch unsere semantische Extraktion entstanden ist präsentieren um die jeweiligen Fachwissenschaftler zur Diskussion über die Entwicklung eines Semantic Web von Keilschriftsprachen
und Keilschriftartefakten einzuladen. Desweiteren soll unser Poster eine Reihe von Anwendungen demonstrieren die sich in Zukunft mit unserer semantischen Ressource entwickeln lassen
können um einen Beitrag zu einem hoffentlich zukünftig existierenden LinkedData Datensatz für
Keilschriftartefakte zur Dokumentation von Keilschrift zu leisten.
},
  url={http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf},
  day={27},
  language={german},
  isbn={978-3-946275-02-2},
  month={February},
  year={2018}
}

@inproceedings{homburg2018integration,
  title={Map Change Prediction for Quality Assurance},
  author={Homburg, Timo and Boochs, Frank and Christophe, Cruz and Roxin, Ana},
  booktitle={LBS 2018},
  abstract={Open geospatial datasources like OpenStreetMap are created by a community of mappers of different experience and with different equipment available. It is therefore important to assess the quality of OpenStreetMap-like maps to give recommendations for users in which situations a map is suitable for their needs. In this work we want to use already defined ways to assess the quality of geospatial data and apply them a features to various Machine Learning algorithms to classify which areas are likely to change in future revisions of the map. In a next step we intend to qualify the changes detected by the algorithm and try to find causes of the changes being tracked.},
  pages={194 - 200},
  publisher={ETH Zurich},
  editor={Kiefer, Peter and Huang, Haosheng and Van de Weghe, Nico and Raubal, Martin},
  doi={10.3929},
  language={english},
  url={https://doi.org/10.3929/ethz-b-000225617},
  booktitle={Adjunct Proceedings of the 14th International Conference on Location Based Services},
  month={January},
  year={2018}
}

@article{homburg2017integration,
  title={Integration, Quality Assurance and Usage of Geospatial Data with Semantic Tools},
  author={Homburg, Timo and Prudhomme, Claire and Boochs, Frank and Christophe, Cruz and Roxin, Ana},
  journal={gis.Science},
  pages={91--96},
  volume={3},
  abstract={In diesem Artikel stellen wir unsere Forschung in der Integration von Geodaten in einen Semantic Web Kontext in unserem Projekt Semantic GIS vor. Zunächst möchten wir den Zweck und die Vorteile einer Integration und Interpretation von Daten in das Semantic Web beleuchten und anschließend unseren Integrationprozess bestehend aus Datengewinnung, automatischer Interpretation, Qualitätssicherung und Provenance sowie den Datenzugriff erklären. Um die Anwendung unserer Forschung zu demonstrieren gehen wir auf zwei Anwendungsfälle in unserem Projekt ein: Die Bewertung von OpenStreetMap Daten und die Verbesserung des Katastrophenschutzes mittels semantischem Reasoning. Wir schließen den Artikel mit einem Fazit, sowie einem kurzen Ausblick auf zukünftige Forschung. 
},
  url={https://gispoint.de/artikelarchiv/gis/2017/gisscience-ausgabe-32017/4201-integration-quality-assurance-and-usage-of-geospatial-data-with-semantic-tools-i-integration-bewertung-und-nutzung-heterogener-datenquellen-mittels-semantischer-werkzeuge-i.html},
  language={german},
  issn={1869-9391},
  month={September},
  year={2017}
}


@inproceedings{homburg2017postagging,
  title={POSTagging and Semantic Dictionary Creation for Hittite Cuneiform},
  author={Homburg, Timo},
  abstract={Presentation Topic and State Of The Art
On	our	poster	we	want	to	present	ongoing	work	to	
create	an	automatic	natural	language	processing	 tool	
for	 Hittite	 cuneiform.	 Hittite	 cuneiform	 texts	 are	 to	
this	 day	 manually	 transcribed	 by	 the	 respective	 experts
 and	 then	 published	 in	 a	 transliteration	 format
(commonly	 ATF). Pictures	 of	 the	 original	 cuneiform	
tablet	 may	 be	 provided	 and	 more	 rarely	 cuneiform	
representations	in	Unicode	are	present.	Due	to	recent	
advancements	in	 the	 field (such	as	Cuneify) an	automatic
 translation	 of	 many	 Hittite	 cuneiform	 transliterations
to	their	respective	cuneiform	representation	
is	possible.
Research Contributions
We	build	upon	this	work	by	creating	tools that	aim
to	 automatically	 translate	 Hittite	 cuneiform	 texts	 to	
English	 from	either	a	Unicode	cuneiform	representation
or	their	transliteration	representation.
POSTagger
We	 have	 created	 a	 morphological	 analyzer	 to	 detect
 nouns,	 verbs,	 several	 kinds	 of	 pronouns,	 their	
respective	 declinations	 and	 appendices	 as	 well	 as	
structural	particles. On	a	sample	set	of	annotated	Hittite
 texts	 from	 different	 epochs	 in	 cuneiform	 and	
transliteration	 representation	 we	 have	 evaluated	 the	
morphological	analyzer,	its	advantages,	problems	and	
possible	solutions	and	intend	to	present	the	results	as	
well	as	 some	 POSTagging	examples	in	 section	 one	 of	
our	poster.
Dictionary Creation
Dictionaries	 for	 Hittite	 cuneiform	 exist	 in	 often	
non-machine	readable	formats	and	without	a	connection
 to	Semantic	Web	 concepts.	We	intend	 to	 change	
this	 situation	 by	 parsing	 digitally	 available	 nonsemantic
dictionaries	and	using	matching	algorithms	to	
find	 concepts	 of	 the	English	 translations	 of	 such	 dictionaries
in	the	Semantic	Web	e.g.	DBPedia	or	Wikidata.
Dictionaries	of	this	kind	are	stored	using	the	Lexical
 Model	 for	 Ontologies	 (Lemon). In	 addition	 to	
freely	 available	 dictionaries	 we	 intend	 to	 use	 expert	
resources developed	 by	 the	 academy	 of	 sciences	 in	
Mainz/Germany to	 verify	 and	 extend	 our	 generated	
dictionaries.	We	intend	to	present	the	dictionary	creation
process,	statistics	about	the	content	of	generated	
dictionaries	 and	 their impact	 in	 section	 two	 of	 our	
poster.
Machine Translation
Using	the	newly	created	dictionaries	as	well	as	the	
POSTagging	information	we	intend	to	test	several	automated
machine	translation approaches	of	which	we	
will	 outline	 the	 process	 and	 possible	 approaches	 in	
poster	section	three.
Contributions for the Communities
With	 our	 approaches	 we	 intend	 to	 contribute to
the	archaeological	community	in	Germany by analysing
 Hittite	 cuneiform	 tablets.	 Together	 with	 work	
from	 the	 University	 of	Heidelberg	 on	image	 recognition
 of	cuneiform	 tablets, we	want	 to	 focus	on	creating
a	natural	language	processing	pipeline	from	scanning
 cuneiform	 tablets	 to	 an	 available	 translation	 in	
English.},
  address = {Montréal, Canada},
  language={english},
  day={9},
  booktitle={Book of Abstracts of DH2017},
  organization={Alliance of Digital Humanities Organizations},
  url={https://dh2017.adho.org/abstracts/139/139.pdf},
  keywords={Hittite, Cuneiform, Dictionary, POSTagging, Semantic Web},
  month={August},
  year={2017}
}

@inproceedings{prudhomme2017automatic,
  title={Automatic Integration of Spatial Data into the Semantic Web},
  author={Prudhomme, Claire and Homburg, Timo and Jean-Jacques, Ponciano and Boochs, Frank and Roxin, Ana and Cruz, Christophe},
  abstract={For several years, many researchers tried to semantically integrate geospatial datasets into the semantic web. Although, there are many general means of integrating interconnected relational datasets (e.g. R2RML), importing schema-less relational geospatial data remains a major challenge in the semantic web community. In our project SemGIS we face significant importation challenges of schema-less geodatasets, in various data formats without relations to the semantic web. We therefore developed an automatic process of semantification for aforementioned data using among others the geometry of spatial objects. We combine Natural Language processing with geographic and semantic tools in order to extract semantic information of spatial data into a local ontology linked to existing semantic web resources. For our experiments, we used LinkedGeoData and Geonames ontologies to link semantic spatial information and compared links with DBpedia and Wikidata for other types of information. The aim of our experiments presented in this paper, is to examine the feasibility and limits of an automated integration of spatial data into a semantic knowledge base and to assess its correctness according to different open datasets. Other ways to link these open datasets have been applied and we used the different results for evaluating our automatic approach.},
  keywords={Geospatial Data, Linked Data, Natural Language Processing, Ontology, R2RML, SDI, Semantic Web, Semantification},
  note={Best Student Paper Award},
  isbn={978-989-758-246-2},
  booktitle={Proceedings of the 13th International Conference on Web Information Systems and Technologies - Volume 1: WEBIST,},
  location = {Porto, Portugal},
  language={english},
  day={26},
  month={April},
  year={2017},
  pages={107-115},
  publisher={SciTePress},
  organization={INSTICC},
  url={http://www.scitepress.org/digitalLibrary/PublicationsDetail.aspx?ID=9PVXQr5fDjQ=&t=1},
  doi={10.5220/0006306601070115}
}

@inproceedings{wurriehausen2016using,
  title={Using an INSPIRE Ontology to Support Spatial Data Interoperability},
  author={W{\"u}rriehausen, Falk and Homburg, Timo and M{\"u}ller, Hartmut},
  booktitle={INSPIRE 2016},
  address = {Barcelona, Spain},
  language={english},
  day={28},
  url={https://inspire.ec.europa.eu/events/conferences/inspire_2016/pdfs/2016_psessions/28%20WEDNESDAY_PSESSIONS_H3_14.00-15.30______28_H3_14.15_188_Presentation.pdf},
  month={September},
  year={2016}
}

@inproceedings{homburg2016interpreting,
  title={Interpreting Heterogeneous Geospatial Data Using Semantic Web Technologies},
  author={Homburg, Timo and Prudhomme, Claire and W{\"u}rriehausen, Falk and Karmacharya, Ashish and Boochs, Frank and Roxin, Ana and Cruz, Christophe},
  booktitle={International Conference on Computational Science and Its Applications},
  pages={240--255},
  day={6},
  month={July},
  year={2016},
  chapter={19},
  isbn={978-3-319-42110-0},
  issn={0302-9743},
  lccn={2016944355},
  abstract={The paper presents work on implementation of semantic technologies within a geospatial environment to provide a common base for further semantic interpretation. The work adds on the current works in similar areas where priorities are more on spatial data integration. We assert that having a common unified semantic view on heterogeneous datasets provides a dimension that allows us to extend beyond conventional concepts of searchability, reusability, composability and interoperability of digital geospatial data. It provides contextual understanding on geodata that will enhance effective interpretations through possible reasoning capabilities. We highlight this through use cases in disaster management and planned land use that are significantly different. This paper illustrates the work that firstly follows existing Semantic Web standards when dealing with vector geodata and secondly extends current standards when dealing with raster geodata and more advanced geospatial operations.},
  doi={10.1007/978-3-319-42111-7},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-42111-7_19},
  editor={Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Rocha, Ana Maria A.C.  and Torre, Carmelo M. and Taniar, David and Apduhan, Bernady O. and Stankova, Elena and Wang, Shangguang},
  address = {Beijing, China},
  language={english},
  organization={Springer International Publishing}
}

@inproceedings{homburg2016word,
  title={Word Segmentation for Akkadian Cuneiform},
  author={Homburg, Timo and Chiarcos, Christian},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
  location = {Portorož, Slovenia},
  address = {Paris, France},
  publisher = {European Language Resources Association (ELRA)},
  abstract={We present experiments on word segmentation for Akkadian cuneiform, an ancient writing system and a language used for about 3 millennia in the ancient Near East. To our best knowledge, this is the first study of this kind applied to either the Akkadian language or the cuneiform writing system. As a logosyllabic writing system, cuneiform structurally resembles Eastern Asian writing systems, so, we employ word segmentation algorithms originally developed for Chinese and Japanese. We describe results of rule-based algorithms, dictionary-based algorithms, statistical and machine learning approaches. Our results may indicate possible promising steps in cuneiform word segmentation that can create and improve natural language processing in this area.},
  isbn={978-2-9517408-9-1},
  url={http://www.lrec-conf.org/proceedings/lrec2016/pdf/816_Paper.pdf},
  editor={Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Goggi,Sara  and Grobelnik,Marko and Maegaard,Bente and Mariani,Joseph and  Mazo,Hélène and Moreno,Asunción and Odijk,Jan and Piperidis,Stelios},
  language = {english},
  month = {May},
  day = {27},
  date = {23-28},
  year={2016}
}

@mastersthesis{homburg2015verfahren,
  title={Verfahren zur Wortsegmentierung nichtalphabetischer Schriften},
  author={Homburg, Timo},
  day={18},
  month={March},
  year={2015},
  language={german},
  abstract={Digital Humanities nowadays gain more and more importance in analyzing texts in new and interesting ways. At the same time, Digital Humanities become recognized by more and more faculties. One example of the application of the Digital Humanities and of computer linguistics can be found in the field of Archaeology. For archaeologists, a decent way of language processing of ancient texts is essential to their work. Many artifacts from ancient civilizations are in need of a textual analysis. Yet many of them have not been analyzed because of a lack of time and human resources or a lack of an automated assisting process for textual analysis. Optimizing the natural language processing chain for an archaeologist can therefore benefit this whole area of expertise. However, in order to analyze texts using a computer, texts have to be broken down to smallest analyzable units, so-called tokens e.g. words. While this has proven to be an easy task to do in European languages like German, French or English (words are usually separated by whitespaces), in non-alphabetic languages like Chinese, Japanese or Korean, word segmentation is a big initial obstacle in analyzing text using a computer. Words in those lan- guages are not clearly separated by distinctive stop chars such as whitespaces. To deal with this obstacle, rulebased, dictionarybased and statistical approaches have been developed for the Chinese and Japanese language. However, to this date no such approaches are known for other nonalphabetic-languages like cuneiform oder Egyptian Hieroglyphs. This Master Thesis applies Chinese and Japanese word segmentation algorithms on the Akkadian language, an ancient language from Mesopotamia written in cuneiform and provides fundamental research in this area. At first, fundamen- tal differences and similarities between Chinese/Japanese and the Akkadian language will be discussed and a model of classification will be introduced. Subsequently, a selection of upto 20 suitable segmentation algorithms adapted from Chinese and Japanese is presented and applied using data from three different epochs of Akkadian history to guarantee representative results. The performance of the algorithms is evaluated afterwards using several evaluation metrics and interpreted to propose further improvements for the segmentation of Akkadian and other related languages.},
  address = {Frankfurt, Germany},
  note={Valedictorian Award Computer Science Master of Science},
  school={Institut f{\"u}r Informatik, Goethe Universit{\"a}t Frankfurt}
}

@misc{homburg2015segmentierung,
  title={Verfahren zur Wortsegmentierung nicht-alphabetischer Schriften},
  author={Homburg, Timo},
  address = {Berlin, Germany},
  publisher = {DH Summit 2015},
  url={https://de.dariah.eu/documents/61689/82910/46_dhaward.pdf/82a466e7-436c-4637-b076-d05171ebd90f},
  note={DARIAH-DE Digital Humanities Award 2015},
  language={german},
  day={3},
  month={March},
  year={2015}
}

@misc{homburg2015learning,
  title={Learning Cuneiform The Modern Way},
  author={Homburg, Timo and Chiarcos, Christian and Richter, Thomas and Wicke, Dirk},
  address = {Graz, Austria},
  publisher = {DHd 2015},
  abstract={Using a poster we
want to propose methods of conveniently typing and learning cuneiform char-
acters, words and phrases for the Akkadian, Sumerian and Hittite language
using input method engines common in Asian languages and by utilising Anki,
a common tool for flash card learning.
Up until this date there is no free and convenient way of typing Unicode cuneiform
characters other than utilizing the Unicode code tables directly. Online dictio-
naries often provide images of cuneiform characters, refer to specific font speci-
fications or do not provide a cuneiform representation at all. More often just a
transliteration or transcription as a valid representation is provided and contrary
to common scientific needs often the only aspect taught in archaelology studies
in universities. Clearly, none of those practices are satisfying or easily adaptable
for text processing and therefore not useful for computeraided teaching methods.
However, an input method engine can act as a suitable tool for solving the men-
tioned input and compatibility problems while at the same time being useful for
education and language learning purposes.
The input method we developed is based on the concept of transliteration input
known from Chinese as Pinyin input, the most common way of typing non-
alphabetical languages on a computer. To achieve an equivalent input for the
aforementioned languages we utilised a given char transliteration to cuneiform
table1 to create transliteration to cuneiform mappings of Akkadian, Sumerian
and Hittite CDLI corpora respectively. Organised as a tree, thus minimising la-
tency, word and charbased input method engines were created for Java (JIME) ,
JQuery, SCIM and Ibus, thereby covering the most important input method
engines on Linux, Web and Java environments.
We furthermore utilized the given data to create flash card sets consisting of
more than 50000 words for the Anki and AnkiDroid7 flash card learning pro-
gram. Anki schedules learning content according to a spaced repetition learning
method having proven its positive learning effect over a longer period of time to
maximize learning success.
Given the two provided concepts teachers can now easily create their own flash
cards according to the pace and content of their lectures. Students may enjoy
a convenient and scientifically proven way of learning cuneiform vocabulary, as
well as a way to prove their learning by utilizing the input method engine to
create their own cuneiform texts. In conclusion, a notable improvement in writ-
ing and in learning the concerned languages has been realisised and is in general
perceived well.},
  booktitle = {Extended Abstract Digital Humanities im deutschsprachigen Raum (DHd 2015)},
  url={http://gams.uni-graz.at/o:dhd2015.p.55},
  language={german},
  day={25},
  month={February},
  year={2015}
}

@inproceedings{homburg2014towards,
  title={Towards workflow planning based on semantic eligibility},
  author={Homburg, Timo and Schumacher, Pol and Minor, Mirjam},
  abstract={A major problem in the research for new artificial intelligence methods for workflows is the evaluation. There is a lack of large evaluation corpora. Existing methods manually model workflows or use workflow extraction to automatically extract workflows from text. Both existing approaches have limitations. The manual modeling of workflows requires a lot of human effort and it would be expensive to create a large test corpus. Workflow extraction is limited by the number of existing textual process descriptions and it is not guaranteed that the workflows are semantically correct. In this paper we suggest to set up a planning domain and apply a planner to create a large number of valid plans. Workflows can be derived from plans. The planner uses a semantic eligibility function to determine whether an operator can be applied to a resource or not. We present a first concept and a prototype implementation in the cooking workflow domain.},
  language={english},
  url={http://wi.cs.uni-frankfurt.de/webdav/publications/puk2014.pdf},
  booktitle={The 37th German Conference on Artificial Intelligence},
  address = {Stuttgart, Germany},
  day={23},
  month={September},
  year={2014}
}

@article{asirwikinect2013,
  title={WikiNect: Gestisches Schreiben f{\"u}r kinetische Museumswikis},
  author={Asir, A and Creech, B and Homburg, Timo and Hoxha, N and R{\"o}hrl, B and Stender, N and Uslu, T and Wiegand, T and Kastrati, L and Valipour, S and others},
  address = {Frankfurt, Germany},
  url={https://hucompute.org/applications/wikinect/},
  school={Institut f{\"u}r Informatik, Goethe Universit{\"a}t Frankfurt},
  year={2013}
}

@bachelorsthesis{homburg2012entwicklung,
  title={Entwicklung einer Androidanwendung zur Zustandsanzeige von Messstationen des Deutschen Wetterdienstes},
  author={Homburg, Timo},
  month={February},
  year={2012},
  language={german},
  address = {Wiesbaden, Germany},
  abstract={This Bachelor thesis describes the development of an Android based mobile application to monitor states of weather monitoring stations for the use of radioactive measurements in the «German National Metrological Service» (DWD). The use case of the application is the scenario of supporting services via mobile phone, which are on duty 24hours. In case of occuring status anomalies, the application should notify the user about the cause of the anomaly and the overall situation in the data collection network.},
  url={https://hds.hebis.de/hsrm/Record/HEB305977989},
  note={Valedictorian Award Computer Science Bachelor of Science},
  school={Hochschule RheinMain}
}
